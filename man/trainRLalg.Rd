% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/trainRLalg.R
\name{trainRLalg}
\alias{trainRLalg}
\alias{plot_trainResult}
\title{The trainRLalg Function}
\usage{
trainRLalg(
  N_train = 10000,
  w_step = 20,
  random.seed = 1234,
  alpha = 0.05,
  lr_theta = 5e-04,
  lr_phi = 5e-04,
  lambda = 2,
  clip_gradient = 10,
  theta0 = NULL,
  theta1 = 0,
  theta2 = -2,
  theta3 = NULL,
  phi1 = 0,
  phi2 = 0.01,
  w = 1.4,
  z = 1.4,
  mu = -0.3,
  sigma = 0.2,
  S0 = 1,
  dt = 1/252,
  rf = 0.02,
  x0 = 1,
  invest_hrzn = 1
)

plot_trainResult(trainRLalg_output, type, ...)
}
\arguments{
\item{N_train}{(Optional, numeric) number of training epochs, default to 10000}

\item{w_step}{(Optional, numeric) number of training epochs to update the Lagrange parameter w, default to 20}

\item{random.seed}{(Optional, numeric) default to 1234}

\item{alpha}{(Optional, numeric) learning rate to update the Lagrange parameter w, default to 0.05}

\item{lr_theta}{(Optional, numeric) learning rate to update \eqn{\theta = (\theta_1, \theta_2)}}

\item{lr_phi}{(Optional, numeric) learning rate to update \eqn{\phi = (\phi_1, \phi_2)}}

\item{lambda}{(Optional, numeric) the exploratory weight, default to 2}

\item{clip_gradient}{(Optional, numeric) clip the gradient within a prespecified range (-`clip_gradient`, +`clip_gradient`)
to help convergence and avoid gradient explosion, default to 10, and can be turned-off if set to NULL}

\item{theta0}{(Optional, numeric) a parameter of the optimal value function, default to NULL and will be
automatically computed as \eqn{\theta_0 = -\theta_2 T^2 - \theta_1 T - (w-z)^2} in the function,
where \eqn{T} is the `invest_hrzn`}

\item{theta1}{(Optional, numeric) a parameter of the optimal value function, default to 0}

\item{theta2}{(Optional, numeric) a parameter of the optimal value function, default to -2}

\item{theta3}{(Optional, numeric) a parameter of the optimal value function, default to NULL and will be
automatically computed as \eqn{\theta_3 = 2\phi_2} in the function}

\item{phi1}{(Optional, numeric) a parameter of the optimal EMV policy, default to 0}

\item{phi2}{(Optional, numeric) a parameter of the optimal EMV policy, default to 0.01}

\item{w}{(Optional, numeric) Lagrange multiplier, default to 1.4}

\item{z}{(Optional, numeric) target terminal wealth, default to 1.4}

\item{mu}{(Optional, numeric) the drift parameter in the stock price process dynamic, default to -0.3}

\item{sigma}{(Optional, numeric) the diffusion parameter in the stock price process dynamic, default to 0.2}

\item{S0}{(Optional, numeric) the initial stock price, default to 1.}

\item{dt}{(Optional, numeric) the discretization of continuous time, default to 1/252,
representing daily discretization. If set to 1/12, then this is monthly discretization.}

\item{rf}{(Optional, numeric) riskfree interest rate, default to 0.02}

\item{x0}{(Optional, numeric) inital wealth, default to 1}

\item{invest_hrzn}{(Optional, numeric) investment horizon, total length of the investment
period in years, default to 1.}

\item{type}{(Required, string) specifies the training result to show, can take value from ("TD_errors", "params", "thetas", "phis")}

\item{(Optional, }{numeric) trainRLalg_output an output object from the `trainRLalg` function, if NULL is given,
it will trigger the `trainRLalg` function and train the RL model.}
}
\value{
A named list containing:
\itemize{
 \item{"params": }{a `N_train`-by-7 matrix containing the training paths of all the parameters
 (`theta_0`, `theta_1`, `theta_2`, `theta_3`, `phi1`, `phi2`, `w`)}
 \item{"wealth_paths": }{`N_train` paths of the portfolio wealth over the investment period}
 \item{"terminal_wealths": }{a vector of length `N_train` containing the terminal wealths of all the wealth paths}
 \item{"investment_paths": }{`N_train` paths of the investment decisions over the investment period}
 \item{"TD_errors": }{a vector of length `N_train` containing the TD errors over the training epochs}
 \item{"dC_dtheta1s": }{a vector of length `N_train` containing the gradient of TD error, computed for updating theta1}
 \item{"dC_dtheta2s": }{a vector of length `N_train` containing the gradient of TD error, computed for updating theta2}
 \item{"dC_dphi1s": }{a vector of length `N_train` containing the gradient of TD error, computed for updating phi1}
 \item{"dC_dphi2s": }{a vector of length `N_train` containing the gradient of TD error, computed for updating phi2}
}
}
\description{
`trainRLalg` trains the RL model and updates parameters for the EMV policy and value function.

`plot_trainResult` plots the training loss (TD_error), the convergence of parameters.
}
