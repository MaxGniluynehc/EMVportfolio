---
title: "Beginners' Handbook of the `EMVportfolio` R package"
author: Yuling Max Chen
output: 
  html_document:
    number_sections: true
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    toc_depth: 3
  pdf_document:
    number_sections: true
    toc: TRUE
    toc_depth: 3
    citation_package: natbib
bibliography: Reference.bib
biblio-style: apalike
reference-section-title: Bibliography
vignette: >
  %\VignetteIndexEntry{Introduction `trimFZC` R package}
  %\VignetteEncoding{UTF-8}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72 
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval=TRUE)
```

The `EMVportfolio` package is designed to solve for the Exploratory Mean-Variance (EMV) portfolio optimization problem, as described in [@wang2020continuous]. In this document, we illustrate the main features of this R package with examples. 


# Background Information
Exploratory Mean-Variance (EMV) portfolio optimization problem [@wang2020continuous] is an investment problem, that aims to maximize the expected portfolio value (wealth) while bearing the minimal risk (variance). According to [@wang2020continuous], the EMV problem is formulated under a Reinforcement Learning (RL) framework, and the optimal exploratory policy follows a Gaussian distribution, from which we can sample the investment decisions (i.e., the dollar value invested in the stock). By parameterizing the mean $\mu$ and standard-deviation $\sigma$ of the Gaussian policy distribution, [@wang2020continuous] constructs a RL training loss, named as the Temporal Difference (TD) error. And the Gaussian policy distribution can be optimized by iteratively updating the parameters and minimizing the TD error. 

For comparison purpose, [@wang2020continuous] also introduced the classical (non-exploratory) Mean-Variance (MV) portfolio optimization problem, which can be solved via Maximum Likelihood Estimation (MLE). 

For more details of the EMV problem, please refer to [@wang2020continuous]. 


# Installation

One can download the latest version of this package from github: 
```{r echo=TRUE, eval=FALSE}
require(devtools)
devtools::install_github("MaxGniluynehc/EMVportfolio",
               ref="main",
               auth_token = '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',
               build_vignettes = TRUE)
library(EMVportfolio)
```


# Overview 

The `EMVportfolio` package has the following architecture: 

```{r echo=FALSE, eval=FALSE, include=FALSE}
PATH = "/Users/maxchen/Documents/Study/STA/STAT900/Writings/Final Project/showdir"
fs::dir_tree(path=PATH, type="any")
```

```{r}
# └── R
#     ├── act_EMV_policy.R
#     ├── act_MLE_policy.R
#     ├── getStockPath.R
#     ├── getTerminalWealth.R
#     ├── show_terminalwealth.R
#     └── trainRLalg.R
```

* The `act_EMV_policy.R` and `act_MLE_policy.R` contains functions that computes the investment decision/action (i.e., dollar value invested in the stock), following either the classical policy with MLE, or the exploratory EMV policy under the RL framework. 
* The `getStockPath.R` simulates a path of stock price process. 
* The `getTerminalWealth.R` trades based on a given policy and computes the terminal wealth, i.e., the portfolio value at the end of the investment period. 
* The `show_terminalwealth.R` visualizes the simulations of terminal wealth with plots and tables.  
* The `trainRLalg.R` optimizes the exploratory policy by training a parameterized RL model and minimizing the TD error. It also contains plotting functions to illustrate the convergence of the training process. 



# Examples

## Simulating Stock Price Paths

One can easily simulate stock price paths by: 
```{r}
set.seed(1234)
N_sim = 100
for (i in 1:N_sim){
  St= getStockPath(mu=-0.3, sigma=0.2, mwindow_size = 0)
  if (i==1){
    par(mai=c(1.02,1,0.4,0.3))
    plot(St, col="gray", type="l", ylim = c(0.3,1.5),
         ylab="Stock Price ($)", xlab = "Time (days)",
         cex.lab=1.5)
  }
  else{
    lines(St, col="gray")
  }
}
```


## Training the RL Model

Training the RL model is simply one line of code, with all arguments taking their default values: 
```{r eval=FALSE}
# For time and presentation purpose, this code chunck is stopped. One can manually set `eval = TRUE` to let it run. 
Out = trainRLalg()
```

The default arguments are chosen following the simulation study in [@wang2020continuous], and it guarantees a converged training path. One can also try to input different values to this function, and see how the convergence of the learning process varies. 

For example, one can change the learning rates (`alpha, lr_theta, lr_phi`) to larger values and let each training epoch update a larger step along the gradient. One can also encourage larger updating steps by setting `clip_gradient` to `NULL`, i.e., remove the restriction on the gradient computation. Besides, one can also manually choose the initial points for the parameters (`theta_0`, `theta_1`, `theta_2`, `theta_3`, `phi1`, `phi2`, `w`). 

To change the market scenario, one can consider changing (`mu`, `sigma`, `S0`, `rf`) to different values. Or, to change the investment period and the continuous-time discretization, one can input different values to (`invest_hrzn`, `dt`). 


### Showing Training Result


## Conduct a Simulation 


## Visualizing Simualtion Results


# Package Testings

## R CMD Checks
As a general checking of the correct design and structure of this package, it is recommended to do conduct a R CMD Check, which can be simply done via the `check()` function in `devtools` package, i.e.,  `devtools::check()`. 

Currently our package passes all the R CMD Checks. 


## Testing for Invalid Inputs










